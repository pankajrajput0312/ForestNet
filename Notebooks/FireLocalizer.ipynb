{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EslzSNByGvNP"
   },
   "source": [
    "#1] Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_THDiREYGmVL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "!pip install -U --pre tensorflow==\"2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mVJ3en_aG1fX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uNlr1XRSG3BE"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-1cb5b0736a81>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-1cb5b0736a81>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    cd models/research/\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Install the Object Detection API\n",
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu9WTzRxG5AK"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import colab_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeNzxD9yG6l4"
   },
   "outputs": [],
   "source": [
    "#Model Builder test\n",
    "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLSEQqTLHCtt"
   },
   "source": [
    "#2] Prepare Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEn9XzhCG8OO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-win_amd64.whl (459.2 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (0.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (3.19.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.19.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.41.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorflow==2.2.0) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\offic\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
      "Installing collected packages: google-auth, tensorflow-estimator, tensorboard, scipy, h5py, gast, tensorflow\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.2\n",
      "    Uninstalling google-auth-2.3.2:\n",
      "      Successfully uninstalled google-auth-2.3.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.2\n",
      "    Uninstalling scipy-1.6.2:\n",
      "      Successfully uninstalled scipy-1.6.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "Successfully installed gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!curl -L \"https://app.roboflow.ai/ds/EVwoZwzA30?key=6OawcH9tOw\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhO5ivntHJEo"
   },
   "outputs": [],
   "source": [
    "test_record_fname = '/content/valid/Smoke.tfrecord'\n",
    "train_record_fname = '/content/train/Smoke.tfrecord'\n",
    "label_map_pbtxt_fname = '/content/train/Smoke_label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_27UeR8LHLPj"
   },
   "outputs": [],
   "source": [
    "MODELS_CONFIG = {\n",
    "    'efficientdet-d0': {\n",
    "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'efficientdet-d1': {\n",
    "        'model_name': 'efficientdet_d1_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'efficientdet-d2': {\n",
    "        'model_name': 'efficientdet_d2_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "        'efficientdet-d3': {\n",
    "        'model_name': 'efficientdet_d3_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n",
    "        'batch_size': 16\n",
    "    }\n",
    "}\n",
    "\n",
    "chosen_model = 'efficientdet-d0'\n",
    "\n",
    "num_steps = 10000 \n",
    "num_eval_steps = 500 \n",
    "\n",
    "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
    "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
    "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
    "batch_size = MODELS_CONFIG[chosen_model]['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmIdnrncHd-P"
   },
   "outputs": [],
   "source": [
    "#download pretrained weights\n",
    "%mkdir /content/models/research/deploy/\n",
    "%cd /content/models/research/deploy/\n",
    "import tarfile\n",
    "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
    "\n",
    "!wget {download_tar}\n",
    "tar = tarfile.open(pretrained_checkpoint)\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwvvFg2iHglJ"
   },
   "outputs": [],
   "source": [
    "#download base training configuration file\n",
    "%cd /content/models/research/deploy\n",
    "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
    "!wget {download_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olZZNMxUHjpw"
   },
   "outputs": [],
   "source": [
    "#prepare\n",
    "pipeline_fname = '/content/models/research/deploy/' + base_pipeline_file\n",
    "fine_tune_checkpoint = '/content/models/research/deploy/' + model_name + '/checkpoint/ckpt-0'\n",
    "\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhFAYsfvHj03"
   },
   "outputs": [],
   "source": [
    "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
    "\n",
    "import re\n",
    "\n",
    "%cd /content/models/research/deploy\n",
    "print('writing custom configuration file')\n",
    "\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open('pipeline_file.config', 'w') as f:\n",
    "    \n",
    "    # fine_tune_checkpoint\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "    \n",
    "    # tfrecord files train and test.\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
    "\n",
    "    # label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "\n",
    "    # Set training batch_size.\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "    \n",
    "    #fine-tune checkpoint type\n",
    "    s = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "        \n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5T6-yipvHr6e"
   },
   "outputs": [],
   "source": [
    "%cat /content/models/research/deploy/pipeline_file.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da3-lSyYHtiz"
   },
   "outputs": [],
   "source": [
    "pipeline_file = '/content/models/research/deploy/pipeline_file.config'\n",
    "model_dir = '/content/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdfxhJIHHcVY"
   },
   "source": [
    "#3] Train Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVA1KRsFH08o"
   },
   "outputs": [],
   "source": [
    "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --num_eval_steps={num_eval_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMivg0iKH2YN"
   },
   "outputs": [],
   "source": [
    "%ls '/content/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv6J36pvH6s3"
   },
   "outputs": [],
   "source": [
    "#run conversion script\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "output_directory = '/content/fine_tuned_model'\n",
    "\n",
    "#place the model weights you would like to export here\n",
    "last_model_path = '/content/training/'\n",
    "print(last_model_path)\n",
    "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir {last_model_path} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zax8OG27H9eQ"
   },
   "outputs": [],
   "source": [
    "%ls '/content/fine_tuned_model/saved_model/'\n",
    "#then download it"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FireLocalizerTrain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
